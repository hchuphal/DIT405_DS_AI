{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assignment5_RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O73TxqjJH7e7"
      },
      "source": [
        "**DAT405 Introduction to Data Science and AI, 2010-2021, Study Period 2** <br/>\n",
        "**Assignment 5: Reinforcement learning and Classification** <br/>\n",
        "**Due Date: Feb 24, 23:59** <br/>\n",
        "\n",
        "**Group 6:**\n",
        "\n",
        "Name | Contribution \n",
        "--- | ---\n",
        "1. Himanshu Chuphal (guschuhi@student.gu.se) | 12 H\n",
        "2. Claudio Aguilar Aguilar(claagu@student.chalmers.se) | 12 H\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "**What to submit**\n",
        "*   **The entire assignment should be submitted through the notebook. No separate file will be accepted.** You can submit either the notebook itself, or a public link to a Google Colab notebook<br/>\n",
        "\n",
        "*In the notebook:*\n",
        "*\tState your names and how many hours each person spent on the assignment.\n",
        "*\tThe solutions and answers to the theoretical and practical problems, including LaTeX math-mode equations, plots and tables etc.\n",
        "*\tAll plots/results should be visible such that the notebook does not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.<br/>\n",
        "\n",
        "*Before submitting:*\n",
        "*   Make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDuY3qwbH7e7"
      },
      "source": [
        "**Self-check**<br/>\n",
        "Is all the required information included? Have you answered all questions to the best of your ability? Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?) Does your notebook run and can reproduce the results, plots and tables?\n",
        "\n",
        "**Grading**<br/>\n",
        "Grading will be based on a qualitative assessment of each assignment. It is important to:\n",
        "*\tPresent clear arguments\n",
        "*\tPresent the results in a pedagogical way\n",
        "*\tShow understanding of the topics (e.g, write a pseudocode) \n",
        "*\tGive correct solutions\n",
        "*\tMake sure that the code is well commented \n",
        "\n",
        "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (Assignment5_Reinforcement_Learning.ipynb) should be submitted with answers and code written in it. No separate file will be accepted.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_6obY12H7e7"
      },
      "source": [
        "# Primer\n",
        "\n",
        "## Decision Making\n",
        "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
        "two parts. First, how do we learn about the world? This involves both the\n",
        "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
        "currently know about the world, how should we decide what to do, taking into\n",
        "account future events and observations that may change our conclusions?\n",
        "Typically, this will involve creating long-term plans covering possible future\n",
        "eventualities. That is, when planning under uncertainty, we also need to take\n",
        "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
        "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
        "known to produce good results and experiment with something new is known\n",
        "as the **exploration-exploitation dilemma**.\n",
        "\n",
        "## The exploration-exploitation trade-off\n",
        "\n",
        "Consider the problem of selecting a restaurant to go to during a vacation. Lets say the\n",
        "best restaurant you have found so far was **Les Epinards**. The food there is\n",
        "usually to your taste and satisfactory. However, a well-known recommendations\n",
        "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n",
        "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
        "in which case you will regret going there. On the other hand, it could also be\n",
        "much better. What should you do?\n",
        "It all depends on how much information you have about either restaurant,\n",
        "and how many more days you’ll stay in town. If this is your last day, then it’s\n",
        "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n",
        "Arm** to be significantly better. However, if you are going to stay there longer,\n",
        "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n",
        "better food for the remaining time, while otherwise you will have missed only\n",
        "one good meal out of many, making the potential risk quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC-SVg0ZH7e7"
      },
      "source": [
        "## Overview\n",
        "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
        "\n",
        "\n",
        "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zZ6HjxpH7e7"
      },
      "source": [
        "## Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurOZxYjH7e7"
      },
      "source": [
        "Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision making under uncertainty. A MDP consists of five parts: the specific decision times, the state space of the environment/system, the available actions for the decision maker, the rewards, and the transition probabilities between the states.\n",
        "\n",
        "* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n",
        "* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n",
        "* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n",
        "* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ for the current state and action, and the resulting next state\n",
        "* Transition probabilities $p(s'|s,a)$ that taking action $a$ in state $s$ will lead to state $s'$\n",
        "\n",
        "At a given decision epoch $t$ and system state $s_t$, the decions maker, or *agent*, chooses an action $a_t$, the system jumps to a new state $s_{t+1}$ according to the transition probability $p(s_{t+1}|s_t,a_t)$, and the agent receives a reward $r_t(s_t,a_t,s_{t+1})$. This process is then repeated for a finite or infinite number of times.\n",
        "\n",
        "A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n",
        "\n",
        "$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n",
        "\n",
        "where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if you think all future rewards should count equally, you would use $\\gamma = 1$, while if you only care less about future rewards you would use $\\gamma < 1$. The expected total *discounted* reward becomes\n",
        "\n",
        "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n",
        "\n",
        "Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^{\\pi^*}(s)$ for all $s\\in S$. That is\n",
        "\n",
        "$$V^{\\pi^*}(s) \\geq V^\\pi(s), s\\in S$$\n",
        "\n",
        "The problem of finding the optimal policy is a _dynamic programming problem_. It turns out that a solution to the optimal policy problem in this context is the *Bellman equation*. The Bellman equation is given by\n",
        "\n",
        "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
        "\n",
        "Thus, it can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n",
        "\n",
        "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
        "\n",
        "A major drawback of MDPs is called the \"Curse of Dimensionality\". MDPs unfortunately do not scale very well with increasing sets of states or actions.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iUmTgzwH7e7"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWPya78-H7e7"
      },
      "source": [
        "In this first question we work with the deterministic MDP, no code is necessary in this part.\n",
        "\n",
        "Setup:\n",
        "\n",
        "* The agent starts in state **S**\n",
        "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n",
        "* Note, that you cannot move outside the grid, thus all actions are not available in every box.\n",
        "* When reaching **F**, the game ends (absorbing state).\n",
        "* The numbers in the boxes represent the rewards you receive when moving into that box. \n",
        "* Assume no discount in this model: $\\gamma = 1$\n",
        "\n",
        "The reward of a state $r(s=(x, y))$ is given by the values on the grid:\n",
        "    \n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|-1 |1|**F**|\n",
        "|0|-1|1|  \n",
        "|-1 |0|-1|  \n",
        "|**S**|-1|1|\n",
        "\n",
        "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n",
        "\n",
        "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n",
        "\n",
        "**[Answer]**\n",
        "The optimal path is **EENNN** since it is unique in the way that it is the shortest path with the sum of 0.\n",
        "\n",
        "But there is also another the path **EENNWNE** that results in a sum of 0 but takes 2 additional steps to do so.\n",
        "\n",
        "\n",
        "**1b)** What is the optimal policy (i.e. the optimal action in each state)?\n",
        "\n",
        "**[Answer]**\n",
        "The optimal policy to complete the optimal path is:\n",
        "\n",
        "1.   When in State(0,0) do E\n",
        "2.   When in State(1,0) do E\n",
        "3.   When in State(2,0) do N\n",
        "4.   When in State(2,1) do N\n",
        "5.   When in State(2,2) do N\n",
        "\n",
        "All other alternative paths until we reach the goal::\n",
        "\n",
        "**(0,0)**: E or N = -1, **(0,1)**: E = 1, **(0,2)** : W or N = -1\n",
        "\n",
        "**(1,0)**: E or N = 0 (return to start S is also possible), **(1,1)**: E or W or N or S = -1, **(1,2)**: N or S = 1 \n",
        "\n",
        "**(2,0)**: E or N or S =-1 , **(2,1)**: E or N = 1, **(2,2)**: N = 0\n",
        "\n",
        "**(3,0)**:  E = 1, **(3,1)**: E = 0, **(3,2)**: No possible action!\n",
        "\n",
        "**1c)** What is expected total reward for the policy in 1b)?\n",
        "\n",
        "**[Answer]**\n",
        "The result that the optimal path **EENNN** sums up to, which is **0**.\n",
        "Here, V(s) is the total sum value we expect to obtain when you follow a current policy starting from initial state (s), which is calculated using following formula: \n",
        "\n",
        "- new value from an action = (currentStateReward + 1 * nextStateReward) \n",
        "*with gamma =1*\n",
        "\n",
        " - V(s) =  expected total reward (added up values for each action)\n",
        "\n",
        "\n",
        "\n",
        "(0,0):N, (1,0):E, (2,0):N, (2,1):N, (2,2):N , (3,2):: **SUM** =-1+0+0+0+1 = 0\n",
        "\n",
        "optimal path from all other alternative paths  from 1(b) ::\n",
        "\n",
        "(0,0):N, (1,0):E, (1,1):E, (1,2):N, (2,2):N, (3,2) :: **sum** = -1-1-1+0+1= -2\n",
        "\n",
        "(0,0):N, (1,0):E, (1,1):N, (2,1):E, (2,2):N, (3,2) :: **sum** = -1-1-1+0+1= -2\n",
        "\n",
        "(0,0):N, (1,0):E, (1,1):N, (2,1):N, (3,1):E, (3,2) :: **sum** = -1-1-1+0+1= -2\n",
        "\n",
        "(0,0):N, (1,0):N, (2,0):E, (2,1):E, (2,2):N, (3,2) :: **sum** = -1-1-1+0+1= -2\n",
        "\n",
        "(0,0):N, (1,0):N, (2,0):E, (2,1):N, (3,1):E, (3,2) :: **sum** = -1-1-1+0+1= -2\n",
        "\n",
        "(0,0):N, (1,0):N, (2,0):N, (3,0):E, (3,1):E, (3,2) :: **sum** = -1-1-1+0+1= -2\n",
        "\n",
        "The other possible paths have worse value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyQ7IatcH7e7"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NfqElM_H7e7"
      },
      "source": [
        "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation. The process of repeated application of the Bellman equation what we here call the _value iteration_ algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qbn4HjqR2fA"
      },
      "source": [
        "The value iteration algorithm practically procedes as follows:\n",
        "\n",
        "```\n",
        "epsilon is a small value, threshold\n",
        "for x from i to infinity \n",
        "do\n",
        "    for each state s\n",
        "    do\n",
        "        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
        "    end\n",
        "    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n",
        "        for each state s,\n",
        "        do\n",
        "            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
        "            return π, V_k \n",
        "        end\n",
        "end\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hOzat7H7e8"
      },
      "source": [
        "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability of 0.8 that that action will be performed and a probability of 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state, that is, we stay on the grid), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|0|0|\n",
        "|0|10|0|  \n",
        "|0|0|0|  \n",
        "\n",
        "\n",
        "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|8|0|\n",
        "|8|2|8|  \n",
        "|0|8|0|  \n",
        "  \n",
        "**Iteration 2**:  \n",
        "  \n",
        "Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \n",
        "Action **S**: 0  \n",
        "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **W**: 0\n",
        "\n",
        "Hence any action between **E** and **N** would be best at this stage.\n",
        "\n",
        "Similarly for cell (1,0):\n",
        "\n",
        "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n",
        "\n",
        "Similar calculations for remaining cells give us:\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|5.76|10.88|5.76|\n",
        "|10.88|8.12|10.88|  \n",
        "|5.76|10.88|5.76|  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdZC73I0DGYP"
      },
      "source": [
        "#imported python modules\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccoMLc71H7e8"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "**2a)** Implement the value iteration algorithm just described here in python, and show the converging optimal value function and the optimal policy for the above 3x3 grid. Hint: use the pseudo-code above as a starting point, but be sure to explain what every line does.\n",
        "\n",
        "**converging optimal value function**\n",
        "\n",
        "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n",
        "\n",
        "\n",
        "\n",
        "**optimal policy (3x3 grid, fulfills bellman equation)**\n",
        "\n",
        "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhoC1jc2CFHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d0cbb2-ebd3-4a8a-c9bd-cff92f3be912"
      },
      "source": [
        "import numpy as np\n",
        "#initial and reward matrix\n",
        "initial_reward =  np.arange(1.0, 10.0).reshape(3,3)\n",
        "reward_matrix =  np.arange(1.0, 10.0).reshape(3,3)\n",
        "grid = np.arange(1.0,10.0).reshape(3,3)\n",
        "#probability of 0.8 that that action will be performed and a probability of 0.2 that no action is taken\n",
        "probAction, probNotAction, Gamma = 0.8, 0.2, 0.9\n",
        "#zero the grid\n",
        "def get_inital_grid():\n",
        "  for i in range (0,3):\n",
        "    for j in range(0,3):\n",
        "      grid[i][j] = 0.0\n",
        "  return grid\n",
        "  \n",
        "print(\"Initial Grid :\")\n",
        "print(get_inital_grid())\n",
        "\n",
        "#3x3 grid.\n",
        "for i in range(0,3):\n",
        "    for j in range (0,3):\n",
        "        initial_reward[i][j] = 0.0\n",
        "initial_reward[1][1]=10.0\n",
        "print(\"Initial Reward :\")\n",
        "print(initial_reward)\n",
        "\n",
        "#size of grid (width x height)\n",
        "grid_width  = grid.shape[0]-1\n",
        "grid_height = grid.shape[1]-1\n",
        "\n",
        "def set_action(grid, x, y, action):\n",
        "  #Define the different actions\n",
        "  #EAST\n",
        "  if (action == 'E'):\n",
        "    if (x+1 <= grid_width and x+1 >= 0): #if within the grid then step to the right\n",
        "      return grid[x+1][y]\n",
        "    else: \n",
        "      return 0 #action not possible\n",
        "  #WEST\n",
        "  if (action == 'W'):\n",
        "    if (x-1 <= grid_width and x-1 >= 0): #if within the grid and >= 0 then step to the left\n",
        "      return grid[x-1][y]\n",
        "    else:\n",
        "      return 0 #action not possible\n",
        "  #SOUTH\n",
        "  if (action == 'S'):\n",
        "    if (y-1 <= grid_height and y-1 >= 0): #if within the grid and >= 0 then step down\n",
        "      return grid[x][y-1]\n",
        "    else:\n",
        "      return 0 #action not possible\n",
        "  #NORTH\n",
        "  if (action == 'N'):\n",
        "    if (y+1 <= grid_height and y+1 >= 0): #if within the grid and >= 0 then step up\n",
        "      return grid[x][y+1]\n",
        "    else:\n",
        "      return 0 #action not possible\n",
        "  #OLDVALUE\n",
        "  if (action == 'O'):\n",
        "    return grid[x][y]\n",
        "\n",
        "#max value as the bellman equation implies as stated in question 2\n",
        "def max_value_action(x, y, current, next):\n",
        "    N = probAction * (set_action(current,x,y,'N')+ Gamma * set_action(next ,x,y,'N'))+ probNotAction * (current[x][y]+ Gamma * next[x][y])\n",
        "    S = probAction * (set_action(current,x,y,'S')+ Gamma * set_action(next ,x,y,'S'))+ probNotAction * (current[x][y]+ Gamma * next[x][y])\n",
        "    E = probAction * (set_action(current,x,y,'E')+ Gamma * set_action(next ,x,y,'E'))+ probNotAction * (current[x][y]+ Gamma * next[x][y])\n",
        "    W = probAction * (set_action(current,x,y,'W')+ Gamma * set_action(next ,x,y,'W'))+ probNotAction * (current[x][y]+ Gamma * next[x][y])\n",
        "    return (max(N,S,E,W))\n",
        "\n",
        "#expected reward for each state in one iteration\n",
        "for i in range(0,3):\n",
        "  for j in range (0,3):\n",
        "    reward_matrix[i][j] = max_value_action(i, j, initial_reward, get_inital_grid())\n",
        "print(\"After 1st Iteration::\")\n",
        "print(reward_matrix)\n",
        "\n",
        "#start the value iteration for total_iteration times\n",
        "def ValueIteration(total_iteration):\n",
        "  final_grid = get_inital_grid() \n",
        "  reward = reward_matrix #expected reward from last iteration\n",
        "  for _ in range(total_iteration-1):\n",
        "    for i in range(0,3):\n",
        "      for j in range(0,3):\n",
        "        final_grid[i][j] = max_value_action(i, j, initial_reward, reward)\n",
        "    reward = final_grid \n",
        "  return final_grid \n",
        "\n",
        "final_grid = ValueIteration(1000)\n",
        "print(\"Value after 1000 Interations ::\")\n",
        "print(final_grid)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Grid :\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Initial Reward :\n",
            "[[ 0.  0.  0.]\n",
            " [ 0. 10.  0.]\n",
            " [ 0.  0.  0.]]\n",
            "After 1st Iteration::\n",
            "[[0. 8. 0.]\n",
            " [8. 2. 8.]\n",
            " [0. 8. 0.]]\n",
            "Value after 1000 Interations ::\n",
            "[[45.61292366 51.94805195 45.61292366]\n",
            " [51.94805195 48.05194805 51.94805195]\n",
            " [45.61292366 51.94805195 45.61292366]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8UCBPJzDDCy"
      },
      "source": [
        "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n",
        "\n",
        "**[Answer]** It is probably because every single state is handeled separately. and applicable actions for each states are independent of each other. Moreover. the initial value $V_0$ consist of small initial reward which will spread out after the 1st iteration. Incase, we have comparetively bigger number in the inital values, then also it would not matter if we increase the total number of iterations to converge the values. The oldest rewards values i.e. the initial value $V_0$ will be taken into account the least after large number of iterations. Beacause of which impact of inital values will be very less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQXoOa7LH7e8"
      },
      "source": [
        "## Reinforcement Learning (RL)\n",
        "Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(a,s,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n",
        "\n",
        "So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n",
        "\n",
        "$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "The value function and the action-value function are directly related through\n",
        "\n",
        "$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n",
        "\n",
        "i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n",
        "\n",
        "$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s\n",
        "]\\,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n",
        "\n",
        "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Q-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n",
        "\n",
        "$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n",
        "\n",
        "where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there is sufficient exploration. While a constant $\\alpha$ generally does not guarantee us to reach true convergence, we keep it constant at $\\alpha=0.1$ for this assignment.\n",
        "\n",
        "## OpenAI Gym\n",
        "\n",
        "We shall use already available simulators for different environments (worlds) using the popular OpenAI Gym library. It just implements [different types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as the [Chain enviroment](https://gym.openai.com/envs/NChain-v0/) illustrated below.\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
        "The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n",
        "\n",
        "## Question 3\n",
        "You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Convergence is **not** a constant value, rather a stable plateau with some noise. Take $\\gamma=0.95$. You can refer to the Q-learning (frozen lake) Jupyter notebook shown in class, uploaded on Canvas. Hint: start with a small learning rate.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYQDTI9soMFU"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCK_81tohSd"
      },
      "source": [
        "Q-learning is one of the most famous model-free reinforcement learning algorithms. It aims to find the optimal policy by maximizing the expected value of the total rewards from the current step to all the following steps. In this sense, Q in Q-learning stands for \"quality\", meaning that the algorithm aims to find the policy with the best quality for the agent. It is described by the Bellman equation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_QuI_0_oigI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "9d7d3ae6-7d7d-4a74-adc3-049c15e345f4"
      },
      "source": [
        "#imported python modules\n",
        "import gym\n",
        "import random\n",
        "import numpy as np \n",
        "from IPython.display import Math, HTML\n",
        "#display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "#               \"latest.js?config=default'></script>\"))\n",
        "\n",
        "Math(r\"NewQ(s,a) = Q(s,a) + \\alpha [R(s,a) + \\gamma max Q(s',a')-Q(s,a)]\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$$NewQ(s,a) = Q(s,a) + \\alpha [R(s,a) + \\gamma max Q(s',a')-Q(s,a)]$$",
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0K1tR7U8wqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c20725d-9801-46c9-d9d3-242d4db51aab"
      },
      "source": [
        "# taken from q_learning_frozen_lake.ipynb\n",
        "env = gym.make('NChain-v0')\n",
        "num_episodes =  1000#15000 #20000 #60000\n",
        "gamma = 0.95 #0.99\n",
        "learning_rate = 0.1 #0.95 #0.85\n",
        "epsilon = 0.5#1 #0.15 #0.1\n",
        "\n",
        "# initialize the Q table\n",
        "Q = np.zeros([5, 2])\n",
        "Q"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aqesBJP9QyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbb358b-9fb6-441f-81bc-7e736a0097a4"
      },
      "source": [
        "## taken from q_learning_frozen_lake.ipynb\n",
        "for _ in range(num_episodes):\n",
        "\tstate = env.reset()\n",
        "\tdone = False\n",
        "\twhile done == False:\n",
        "        # First we select an action:\n",
        "\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
        "\t\t\taction = env.action_space.sample() # Explore action space\n",
        "\t\telse:\n",
        "\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n",
        "        # Then we perform the action and receive the feedback from the environment\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
        "\t\tQ[state,action] += learning_rate*update \n",
        "\t\tstate = new_state\n",
        "#Printing the Q table\n",
        "Q"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[62.73119814, 61.61184087],\n",
              "       [66.77077242, 61.8414113 ],\n",
              "       [71.61108039, 62.89457625],\n",
              "       [76.54452395, 62.27858727],\n",
              "       [81.55990703, 67.58229107]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xRl95FNmlrs"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "**4a)** Define the MDP corresponding to the Chain environment above and verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action. Hint: compare values obtained using value iteration and Q-learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6KkEya1euoh",
        "outputId": "348f6829-4a40-437a-9a7b-795d6293af32"
      },
      "source": [
        "#Using Figure 1 states and rewards \n",
        "states = [1,2,3,4,5] #total 5 states\n",
        "reward_b = 2. # for backward\n",
        "reward_f = 0. # for forward\n",
        "reward_f5a = 10. #for state 5\n",
        "gamma = 0.95 # as given in the previous question 3\n",
        "#expected reward for each state in one iteration\n",
        "initial_val = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "#updating max value action from question 2\n",
        "def max_value_action_2(state, next): # based on Figure 1\n",
        "    # for states 1 to 4\n",
        "    if(state >= 0 and state <4): \n",
        "      forward_reward = probAction * (reward_f + gamma * next[state+1])+ probNotAction * (reward_b + gamma * next[0])\n",
        "      backward_reward = probAction * (reward_b + gamma * next[0])+ probNotAction * (gamma * next[state+1])\n",
        "    #for state 5, the loop starts from 0\n",
        "    if (state == 4):\n",
        "      forward_reward = probAction * (reward_f5a + gamma * next[state])+ probNotAction * (reward_b + gamma * next[0])\n",
        "      backward_reward = probAction * (reward_b + gamma * next[0])+ probNotAction * (reward_f5a + gamma * next[state])\n",
        "    return max(forward_reward, backward_reward)\n",
        "\n",
        "#An inital value iteration is made\n",
        "reward = initial_val\n",
        "for i in range(0,5):\n",
        "  reward[i] = max_value_action_2(i, initial_val)\n",
        "print(\"After 1st Iteration:\")\n",
        "print(reward)\n",
        "\n",
        "#start the value iteration for total_iteration times\n",
        "def ValueIteration(total_iteration):\n",
        "  final_reward = initial_val\n",
        "  reward_p = reward #from 1st iteration\n",
        "  for _ in range(total_iteration-1):\n",
        "    for i in range(0, len(states)):\n",
        "      final_reward[i] = max_value_action_2(i, reward_p)\n",
        "    reward_p = final_reward\n",
        "  return final_reward\n",
        "\n",
        "final_value = ValueIteration(1000)\n",
        "print(\"Value after 1000 Interations ::\")\n",
        "print(final_value)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 1st Iteration:\n",
            "[1.6, 2.8160000000000003, 2.8160000000000003, 2.8160000000000003, 8.704]\n",
            "Value after 1000 Interations ::\n",
            "[61.37948159999994, 64.89128959999994, 69.51208959999994, 75.59208959999994, 83.59208959999992]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYCtIsLIydax"
      },
      "source": [
        "- Value obtained from from 3 ( **Q-learning**)::\n",
        "\n",
        "        [[62.73119814, 61.61184087],\n",
        "       [66.77077242, 61.8414113 ],\n",
        "       [71.61108039, 62.89457625],\n",
        "       [76.54452395, 62.27858727],\n",
        "       [81.55990703, 67.58229107]\n",
        "\n",
        "- Value obtained from 4a ( **value Interation** )::\n",
        "\n",
        "    [61.37948159999994, \n",
        "\n",
        "    64.89128959999994, \n",
        "\n",
        "    69.51208959999994,\n",
        "\n",
        "    75.59208959999994,\n",
        "    \n",
        "    83.59208959999992]\n",
        "\n",
        "- the result of both simple q-learning (3a) and  value iteration algoritm (4a) look almost the same and the slight difference in the values we belive could be beacause of default value of parameters used in 3a (for exploration and exploitation). Also, the total number of interations for learning rate could also make a difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgJvwB4levAZ"
      },
      "source": [
        "\n",
        "**4b)** What is the importance of exploration in RL? Explain with an example.\n",
        "\n",
        "**[Answer]** Exploration is an important component of reinforcement learning (RL). As reinforcement learning scales up to address increasingly complex problems, efficient exploration is increasingly important.\n",
        "\n",
        "\n",
        "Exploration is important in order to find out new actions that could in the long run lead to higher rewards. A real life example could be when choosing which restaurant to go to. One option is a restaurant that you are familiar with which you always enjoy. The second option is another restaurant that you have heard serve very tasty food. Now this leaves you with the choice to either go to the first restaurant that you know will leave you satisfied(**exploit**) or try the other restaurant(**explore**) that might even end up being better than the first one. Do you risk trying the new restaurant? the outcomes could be that you are not as satisfied as when you visit the first restaurant or another outcome could be that you found a better restaurant and now you will be visiting this one instead of the first one, which in long term will be helpful because now you have found a better restaurant that you would not have found if you did not decide to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWUqaN60H7e8"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n",
        "\n",
        "**[Answer]** A decision tree is a class of *regression and classification* tasks which can be represented with a *tree* structured graph. Decision tree algorithm belongs to the family of supervised learning algorithms, used for solving regression and classification problems.\n",
        "\n",
        "The nodes, branches and leaves in a decision tree represent questions, answers, labels respectively, starting with the **root** (the 1st question) of the tree. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. For each level in the tree, a new question is asked and subsequently, a new branch is added and a new area in the feature space is split off. We continue doing this until a set number of splits has been achieved. A decision tree splits the nodes on all available variables and accordingly selects the split, resulting in most homogeneous sub-nodes, which affects a tree’s accuracy. Decision trees use multiple algorithms/methods to decide to split a node into two or more sub-nodes. The purity of the node increases with respect to the target variable and the algorithm selection is also based on the type of target variables.\n",
        "\n",
        "**5b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.\n",
        "\n",
        "**[Answer]** Supervised learning is when a model learns from a labeled data-set with some guidance and the Reinforcement-Learning is when a machine or an agent interacts with its environment, performs actions, and learns by a trial-and-error method i.e. the RL agent doens't know it's environment, but can interact with it to learn and improve. \n",
        "\n",
        "Comparing Supervised learning tasks such as *regression or classification*,  as the model learns from a labeled data-set i.e. we will need to have the previous understanding of the environment so as to get the best performance and also doesn't include exploration. In Reinforcement-Learning, it is approached by *learns by a trial-and-error method*, making is more independent approach as compared to supervised learning approach. The RL learning agent interacts with\n",
        "the environment by executing actions and learns from errors/rewards.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wev-_UhcH7e8"
      },
      "source": [
        "\n",
        "# References\n",
        "Primer/text based on the following references:\n",
        "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
        "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
      ]
    }
  ]
}