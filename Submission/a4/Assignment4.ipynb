{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "source": [
    "# DAT405 Introduction to Data Science and AI \n",
    "**Group 6:**\n",
    "\n",
    "Name | Contribution \n",
    "--- | ---\n",
    "1. Himanshu Chuphal (guschuhi@student.gu.se) | 10 H\n",
    "2. Claudio Aguilar Aguilar(claagu@student.chalmers.se) | 10 H\n",
    "\n",
    "---\n",
    "## Assignment 4: Spam classification using Naïve Bayes \n",
    "\n",
    "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
    "Hints:\n",
    "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
    "\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
    "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
    "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "-\thard-ham: non-spam messages more difficult to differentiate \n",
    "-\tspam: spam messages \n",
    "\n",
    "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use 7zip (https://www.7-zip.org/download.html) to decompress the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wa37fBwRF-xe",
    "outputId": "80e13bfd-5b25-49ee-fbee-b799988529f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-16 20:13:42--  https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 207.244.88.140, 95.216.26.30, 2a01:4f9:2a:1a61::2\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|207.244.88.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1677144 (1.6M) [application/x-bzip2]\n",
      "Saving to: ‘20021010_easy_ham.tar.bz2.3’\n",
      "\n",
      "20021010_easy_ham.t 100%[===================>]   1.60M  3.74MB/s    in 0.4s    \n",
      "\n",
      "2021-02-16 20:13:42 (3.74 MB/s) - ‘20021010_easy_ham.tar.bz2.3’ saved [1677144/1677144]\n",
      "\n",
      "--2021-02-16 20:13:42--  https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 95.216.26.30, 207.244.88.140, 2a01:4f9:2a:1a61::2\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|95.216.26.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1021126 (997K) [application/x-bzip2]\n",
      "Saving to: ‘20021010_hard_ham.tar.bz2.3’\n",
      "\n",
      "20021010_hard_ham.t 100%[===================>] 997.19K   972KB/s    in 1.0s    \n",
      "\n",
      "2021-02-16 20:13:44 (972 KB/s) - ‘20021010_hard_ham.tar.bz2.3’ saved [1021126/1021126]\n",
      "\n",
      "--2021-02-16 20:13:44--  https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 207.244.88.140, 95.216.26.30, 2a01:4f9:2a:1a61::2\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|207.244.88.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1192582 (1.1M) [application/x-bzip2]\n",
      "Saving to: ‘20021010_spam.tar.bz2.3’\n",
      "\n",
      "20021010_spam.tar.b 100%[===================>]   1.14M  2.77MB/s    in 0.4s    \n",
      "\n",
      "2021-02-16 20:13:45 (2.77 MB/s) - ‘20021010_spam.tar.bz2.3’ saved [1192582/1192582]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download and extract data\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "!tar -xjf 20021010_easy_ham.tar.bz2\n",
    "!tar -xjf 20021010_hard_ham.tar.bz2\n",
    "!tar -xjf 20021010_spam.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A53Gw00fBLG2",
    "outputId": "56c6192b-ffd2-4ff2-877d-65c706b16228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16M\n",
      "drwxr-xr-x 1 root root 4.0K Feb 16 20:13 .\n",
      "drwxr-xr-x 1 root root 4.0K Feb 16 18:34 ..\n",
      "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2\n",
      "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.1\n",
      "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.2\n",
      "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.3\n",
      "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2\n",
      "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2.1\n",
      "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2.2\n",
      "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2.3\n",
      "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2\n",
      "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2.1\n",
      "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2.2\n",
      "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2.3\n",
      "drwxr-xr-x 1 root root 4.0K Feb 10 14:40 .config\n",
      "drwx--x--x 2  500  500 184K Oct 10  2002 easy_ham\n",
      "drwx--x--x 2 1000 1000  20K Dec 16  2004 hard_ham\n",
      "drwxr-xr-x 1 root root 4.0K Feb 10 14:40 sample_data\n",
      "drwxr-xr-x 2  500  500  36K Oct 10  2002 spam\n"
     ]
    }
   ],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "###1. Preprocessing: \n",
    "1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that and run on the entire text. \n",
    "2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) **0.5p**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J2sllUWXKblD"
   },
   "outputs": [],
   "source": [
    "#pre-processing code here\n",
    "#imported python modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oI1XIFDqa02e",
    "outputId": "45a4fcb4-54b4-44d1-d96b-d3d82911c661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files in each directory: hard_ham_files: 250, easy_ham_files: 2551, spam_files: 501 \n"
     ]
    }
   ],
   "source": [
    "#common file handling for shared data\n",
    "ROOT = os.getcwd()\n",
    "EASY_HAM_PATH = os.path.join(ROOT, \"easy_ham\")\n",
    "HARD_HAM_PATH = os.path.join(ROOT, \"hard_ham\")\n",
    "SPAM_PATH = os.path.join(ROOT, \"spam\")\n",
    "# get files for each folder\n",
    "easy_ham_files = [fname for fname in sorted(os.listdir(EASY_HAM_PATH))]\n",
    "hard_ham_files = [fname for fname in sorted(os.listdir(HARD_HAM_PATH))]\n",
    "spam_files = [fname for fname in sorted(os.listdir(SPAM_PATH))]\n",
    "print('Total Files in each directory: hard_ham_files: {:d}, easy_ham_files: {:d}, spam_files: {:d} '.format(len(hard_ham_files),\n",
    "                                          len(easy_ham_files),\n",
    "                                          len(spam_files)))\n",
    "#function to decode the email files\n",
    "def get_emails(files, directory, message_type):\n",
    "    emails  = []\n",
    "    for filename in files:\n",
    "      if filename is not None:\n",
    "        with open(os.path.join(ROOT, directory, filename), \"rb\") as f:\n",
    "          email = f.read()\n",
    "          #decode the email with labels for each type of message\n",
    "          emails.append({'message': email.decode('latin-1'),\n",
    "                         'message_type': message_type })\n",
    "        f.close()\n",
    "    return pd.DataFrame(emails) #return the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7pMbWDyJ2ip",
    "outputId": "307fc4cd-ccc2-4a44-e173-a8b69c8d7d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails: 3302\n",
      "Ham :: train data set2641, test set: 661 \n",
      "Spam :: train data set 2641, test set: 661 \n"
     ]
    }
   ],
   "source": [
    "#get data frames\n",
    "df1 = get_emails(easy_ham_files, \"easy_ham\", \"ham\")\n",
    "df2 = get_emails(hard_ham_files, \"hard_ham\", \"ham\")\n",
    "df3 = get_emails(spam_files, \"spam\", \"spam\")\n",
    "frames = [df1, df2, df3] # concatnating all the emails data frames\n",
    "all_mail_data = pd.concat(frames)\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(all_mail_data['message'],\n",
    "                                                          all_mail_data['message_type'],\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state=1)\n",
    "print('Total emails:',all_mail_data.shape[0])\n",
    "print('Ham :: train data set{:d}, test set: {:d} '.format(hamtrain.shape[0], hamtest.shape[0]))\n",
    "print('Spam :: train data set {:d}, test set: {:d} '.format(spamtrain.shape[0], spamtest.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "###2. Write a Python program that: \n",
    "1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
    "2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n",
    "    - Multinomial Naive Bayes  \n",
    "    - Bernoulli Naive Bayes. \n",
    "\n",
    "\n",
    "    - updated with True Positive and True Negative\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MJERHSCcGNaW"
   },
   "outputs": [],
   "source": [
    "#Code here\n",
    "email_fit = CountVectorizer().fit_transform(all_mail_data['message'])\n",
    "#Learn the vocabulary dictionary from the messages and return document-term matrix.\n",
    "y = all_mail_data['message_type']\n",
    "# splitting the data into 20% test and 80% train data\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEhiS8T7C3hX"
   },
   "source": [
    "#Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVpUVmVaC4n7",
    "outputId": "fb8ef1b1-241a-4624-cc99-7b1e70e68a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multinomial Naive Bayes Model Classifer::\n",
      "True Positive : 0.897 \n",
      "True Negative: 0.991 \n",
      "False Positive : 0.009 \n",
      "False Negative: 0.103 \n"
     ]
    }
   ],
   "source": [
    "#2 Multinomial Naive Bayes\n",
    "def MNB_model(hamtrain, spamtrain, hamtest, spamtest):\n",
    "  print(\"\\nMultinomial Naive Bayes Model Classifer::\")\n",
    "  MNB = MultinomialNB()\n",
    "  MNB.fit(hamtrain, spamtrain)\n",
    "  MNB_predict = MNB.predict(hamtest) \n",
    "  #binary classification, the count of true negatives , false negatives, true positives and false positives. \n",
    "  tn, fp, fn, tp = confusion_matrix(spamtest,\n",
    "                                    MNB_predict,\n",
    "                                    normalize='true').ravel()# from matrix to array\n",
    "  matrix = classification_report(spamtest, MNB_predict)\n",
    "  print(\"True Positive : {:.3f} \".format(tp)) \n",
    "  print(\"True Negative: {:.3f} \".format(tn))  \n",
    "  #printing False rates too\n",
    "  print(\"False Positive : {:.3f} \".format(fp)) \n",
    "  print(\"False Negative: {:.3f} \".format(fn)) \n",
    "\n",
    "MNB_model(hamtrain, spamtrain, hamtest, spamtest)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1JQAErwA9Pu"
   },
   "source": [
    "# Bernoulli Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLgYLvWvA4SM",
    "outputId": "52fae4bf-dd5e-4198-cdd2-a5f48a979d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bernoulli Naive Bayes Classifer::\n",
      "True Positive : 0.320 \n",
      "True Negative: 0.989 \n",
      "False Positive : 0.011 \n",
      "False Negative: 0.680 \n"
     ]
    }
   ],
   "source": [
    "# 2 Bernoulli Naive Bayes.\n",
    "def BNB_model(hamtrain, spamtrain, hamtest, spamtest):\n",
    "  print(\"\\nBernoulli Naive Bayes Classifer::\")\n",
    "  BNB = BernoulliNB()\n",
    "  BNB.fit(hamtrain, spamtrain)\n",
    "  BNB_predict = BNB.predict(hamtest)\n",
    "  #binary classification, the count of true negatives, false negatives, true positives and false positives. \n",
    "  tn, fp, fn, tp = confusion_matrix(spamtest,\n",
    "                                    BNB_predict,\n",
    "                                    normalize='true').ravel()# from matrix to array\n",
    "  matrix = classification_report(spamtest, BNB_predict)\n",
    "  print(\"True Positive : {:.3f} \".format(tp)) \n",
    "  print(\"True Negative: {:.3f} \".format(tn))  \n",
    "  print(\"False Positive : {:.3f} \".format(fp)) \n",
    "  print(\"False Negative: {:.3f} \".format(fn)) \n",
    "\n",
    "BNB_model(hamtrain, spamtrain,  hamtest, spamtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu33NshxIwVs"
   },
   "source": [
    "**a) Explain how the classifiers differ. What different interpretations do they \n",
    "have?** 1p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI1bPDCvQxen"
   },
   "source": [
    "- sklearn.metrics.confusion_matrix helps to evaluate the accuracy of a classifications and Normalize flag Normalizes confusion matrix over the true (rows), predicted (columns) conditions.\n",
    "\n",
    "- In general, we can say that a better model algorithm makes good prediction should indicate::\n",
    "\n",
    " 1) always give high rates for True positive and True Negatives, and\n",
    " \n",
    " 2) low values for False Positives and False Negatives \n",
    " \n",
    "Having said that, as per the results we got before for MNB (Multinomial Naive Bayes) and BNB (Bernoulli Naive Bayes model) models, we can say that MNB model does better on predicting true positive rates and true negatives in the case ham emails. \n",
    "\n",
    "MNB model predictions gave less false rates, i.e it is better on predicting the spam emails as well. \n",
    "\n",
    "On the other hand, the BNB model did poor at predicting true positives (0.32) and gave a high value for False Negative(0.68).\n",
    "\n",
    "\n",
    "**MNB vs BNB:**\n",
    "\n",
    "In general the differences between the two classifiers are:\n",
    "\n",
    "**Multinomial Naive Bayes** considers multiple features and how many times they occur in the dataset. **Bernoulli Naive Bayes** only considers a single feature and how many times that single feature occurs and does not occur in the dataset.\n",
    "\n",
    "The **Multinomial Naive Bayes** classifier will classify the emails based on the counts of multiple keywords(ham,spam), while **Bernoulli Naive Bayes** will only focus on the counts of a single keyword(ham or spam) and also how many times that single keyword does not occur in the mails.\n",
    "\n",
    "This leads to the **Bernoulli Naive Bayes** classifier penalising an email that does not contain a specific feature, which leads to less true positives which can be seen in the prints.\n",
    "\n",
    "Hence, **Multinomial Naive Bayes** classifier comes in handy\n",
    "When you have multiple features to worry about while **Bernoulli Naive Bayes** is best suited when there is only one feature to worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3.Run your program on \n",
    "-\tSpam versus easy-ham \n",
    "-\tSpam versus (hard-ham + easy-ham). \n",
    "-   Discuss your results **2.5p** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gool_zb8Qzzy",
    "outputId": "396676fb-9156-4d7c-a48e-78ad2620fb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam versus easy-ham Rates\n",
      "\n",
      "Multinomial Naive Bayes Model Classifer::\n",
      "True Positive : 0.935 \n",
      "True Negative: 0.998 \n",
      "False Positive : 0.002 \n",
      "False Negative: 0.065 \n",
      "\n",
      "Bernoulli Naive Bayes Classifer::\n",
      "True Positive : 0.565 \n",
      "True Negative: 0.996 \n",
      "False Positive : 0.004 \n",
      "False Negative: 0.435 \n",
      "\n",
      "Spam versus (hard-ham + easy-ham Rates)\n",
      "\n",
      "Multinomial Naive Bayes Model Classifer::\n",
      "True Positive : 0.897 \n",
      "True Negative: 0.991 \n",
      "False Positive : 0.009 \n",
      "False Negative: 0.103 \n",
      "\n",
      "Bernoulli Naive Bayes Classifer::\n",
      "True Positive : 0.320 \n",
      "True Negative: 0.989 \n",
      "False Positive : 0.011 \n",
      "False Negative: 0.680 \n"
     ]
    }
   ],
   "source": [
    "#Code to report results here\n",
    "#Spam versus easy-ham\n",
    "frames_spam_easy_ham = [df1, df3] # df1 -> easy_ham and df3->spam\n",
    "all_mail_data = pd.concat(frames_spam_easy_ham)\n",
    "email_fit = CountVectorizer().fit_transform(all_mail_data['message'])    \n",
    "y = all_mail_data['message_type']\n",
    "#using the same test split as before\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state = 1)\n",
    "\n",
    "#MNB\n",
    "print(\"Spam versus easy-ham Rates\")\n",
    "MNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "#BNB\n",
    "BNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "\n",
    "\n",
    "print()\n",
    "#Spam versus (hard-ham + easy-ham)\n",
    "print(\"Spam versus (hard-ham + easy-ham Rates)\")\n",
    "# df1 -> easy_ham and df3->spam and df2->hard_ham\n",
    "frames_spam_easy_ham_hard_ham = [df1, df2, df3]\n",
    "all_mail_data = pd.concat(frames_spam_easy_ham_hard_ham)\n",
    "email_fit = CountVectorizer().fit_transform(all_mail_data['message'])    \n",
    "y = all_mail_data['message_type']\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state = 1)\n",
    "\n",
    "#MNB\n",
    "MNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "\n",
    "#BNB\n",
    "BNB_model(hamtrain, spamtrain,  hamtest, spamtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRJk5qBYYk2u"
   },
   "source": [
    "#Discussion\n",
    "\n",
    "- Similar to previous discussion and based on results from the previous cell, we can say that the **Bernoulli Naive Bayes model** performed better this time at predicting both ham (higher True Positive : 0.565) and spam (lower False Negative: 0.435) in case Spam versus easy-ham : probably because the model got higher *proportion* of the training data which was spam emails and got less ham message-type to train on.\n",
    "\n",
    "\n",
    "- Comparing **Multinomial Naive Bayes model**, which gave us better rates before as well and again this time there was not much of the difference between comparing rates in spam vs. easy-ham and Spam versus (hard-ham + easy-ham Rates) and we got slightly better TP and TN rates in case Spam versus easy-ham Rates (probably because of the same reason as mentioned in the previous explanation).\n",
    "\n",
    "- Also, seems like when both the models only got to train on the easy-spam, all the rates got somewhat better, hard-ham are non-spam messages which more difficult to differentiate. Therefore, probably the models performed better in differentiating with Spam versus easy-ham scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkfQWBB4UhYd"
   },
   "source": [
    "# 4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n",
    "\n",
    "**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. **1p** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPj3wMbrlKzw",
    "outputId": "0642e2b9-bf32-40dd-f1e6-d2ed2e02a02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of the 10 most common words:\n",
      "com 69898\n",
      "the 40824\n",
      "to 38179\n",
      "http 34048\n",
      "from 28715\n",
      "td 28399\n",
      "2002 28275\n",
      "3d 25415\n",
      "for 23845\n",
      "net 22839\n",
      "\n",
      "\n",
      "list of the 10 most uncommon words:\n",
      "bm7 1\n",
      "5999 1\n",
      "xbr 1\n",
      "bowled 1\n",
      "vascular 1\n",
      "cardio 1\n",
      "aranthopanacis 1\n",
      "pseudoslellariae 1\n",
      "corni 1\n",
      "mouton 1\n"
     ]
    }
   ],
   "source": [
    "#find the words that are too common\n",
    "def count_common_uncommon_words(message, n=None):\n",
    "  vector = CountVectorizer().fit(message) #fit the dataset\n",
    "  matrix_of_words = vector.transform(message) \n",
    "  # each word divided into columns for each row(file in the dataset) with their occurence in each file.\n",
    "  sum_of_words = matrix_of_words.sum(axis=0)\n",
    "  # sum for each column(each occurence for a word in all files)\n",
    "  frequency_of_words = [(word, sum_of_words[0, index]) \n",
    "                       for word, index in vector.vocabulary_.items()]\n",
    "  frequency_of_words_sorted = sorted(frequency_of_words, key=lambda x: x[1], reverse=True)\n",
    "  # sort the list of occurences for each word in descending order\n",
    "  return frequency_of_words_sorted[:n]\n",
    "\n",
    "# COMMON WORDS\n",
    "print(\"list of the 10 most common words:\")\n",
    "#10 most common words in messages\n",
    "common_words = count_common_uncommon_words(all_mail_data['message'], 10)\n",
    "#loop through the most common words and print them out with their nr of occurences\n",
    "for (word, frequency) in common_words:\n",
    "  print(word, frequency)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# UNCOMMON WORDS\n",
    "print(\"list of the 10 most uncommon words:\")\n",
    "uncommon_words = count_common_uncommon_words(all_mail_data['message'])\n",
    "#loop through the most uncommon words and print them out with their nr of occurences\n",
    "for (word, frequency) in uncommon_words[:-11:-1]: # take the 10 last elements backwards\n",
    "  print(word, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5fkuSfRW4lA"
   },
   "source": [
    "**In General:**\n",
    "\n",
    "By filtering out these words it could lead to better predictions since we are cutting the amount of data and training the model on more relevant words. Common words can be hard to classify since they appear in almost every mail and can mislead the calculation.  \n",
    "\n",
    "**Problem with uncommon words:**\n",
    "\n",
    "When the training set contains an uncommon word that only appear in one message type(ham emails) and no times in the other one(spam emails). Then the model would be trained to interpret this uncommon word as spam in this case.\n",
    "\n",
    "**Problem with common words:**\n",
    "\n",
    "Having alot of common words that appear in almost every email makes it harder to classify an email. Therefore it is a good idea to filter these out because they could otherwise mislead the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_yXfLImlH-3"
   },
   "source": [
    "**b.** **Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report and discuss your results. You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making.** **1p** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AP-c1Fxskmqc",
    "outputId": "89c46663-3cb6-4694-ec5f-f52ddcdd4616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM VS EASY-HAM\n",
      "\n",
      "Multinomial Naive Bayes Model Classifer::\n",
      "True Positive : 0.954 \n",
      "True Negative: 0.938 \n",
      "False Positive : 0.062 \n",
      "False Negative: 0.046 \n",
      "\n",
      "\n",
      "Bernoulli Naive Bayes Classifer::\n",
      "True Positive : 0.981 \n",
      "True Negative: 0.893 \n",
      "False Positive : 0.107 \n",
      "False Negative: 0.019 \n"
     ]
    }
   ],
   "source": [
    "#lets go for the method were sklearn does it for us\n",
    "#SPAM VS EASY-HAM\n",
    "frames_spam_easy_ham = [df1, df3] # df1 -> easy_ham and df3->spam\n",
    "all_mail_data = pd.concat(frames_spam_easy_ham)\n",
    "#remove words that appear are in less than 10% and more than 80% of the messages. \n",
    "email_fit = CountVectorizer(min_df=0.1, max_df=0.8).fit_transform(all_mail_data['message'])    \n",
    "y = all_mail_data['message_type']\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state =1)\n",
    "#MNB\n",
    "print(\"SPAM VS EASY-HAM\")\n",
    "MNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "#BNB\n",
    "print()\n",
    "BNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ng6I45luVrp",
    "outputId": "ed23e13c-c2f4-4171-8a66-260ef773def0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam versus (hard-ham + easy-ham Rates)\n",
      "SPAM VS (HARD-HAM + EASY-HAM)\n",
      "\n",
      "Multinomial Naive Bayes Model Classifer::\n",
      "True Positive : 0.918 \n",
      "True Negative: 0.929 \n",
      "False Positive : 0.071 \n",
      "False Negative: 0.082 \n",
      "\n",
      "\n",
      "Bernoulli Naive Bayes Classifer::\n",
      "True Positive : 0.969 \n",
      "True Negative: 0.858 \n",
      "False Positive : 0.142 \n",
      "False Negative: 0.031 \n"
     ]
    }
   ],
   "source": [
    "#SPAM VS (HARD-HAM + EASY-HAM)\n",
    "#Spam versus (hard-ham + easy-ham). - Already done\n",
    "print(\"Spam versus (hard-ham + easy-ham Rates)\")\n",
    "# df1 -> easy_ham and df3->spam and df2->hard_ham\n",
    "frames_spam_easy_ham_hard_ham = [df1, df2, df3]\n",
    "all_mail_data = pd.concat(frames_spam_easy_ham_hard_ham)\n",
    "#remove words that appear are in less than 10% and more than 80% of the messages. \n",
    "email_fit = CountVectorizer(min_df=0.1, max_df=0.8).fit_transform(all_mail_data['message'])    \n",
    "y = all_mail_data['message_type']\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state =1)\n",
    "\n",
    "#MNB\n",
    "print(\"SPAM VS (HARD-HAM + EASY-HAM)\")\n",
    "MNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "#BNB\n",
    "print()\n",
    "BNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1Zouh2_8giL"
   },
   "source": [
    "**Discussion of the results**\n",
    "\n",
    "**Spam Vs Easy Ham:**\n",
    "After using Sklearn’s CountVectorizer to filter out uncommon/common words, both the models performed better. **MNB:** did even better in terms of true positives and true negatives. But we see major improvements in case of **BNB:**, the Binomial Naive Bayes classifier perfomed much better in true positives and better in False positives (True Positive : 0.981, True Negative: 0.893 )\n",
    "\n",
    "**Spam versus (hard-ham + easy-ham)\n",
    ":**\n",
    "\n",
    "We see the same improvements in this case as well for both the models. Both **MNB** and **BNB** did well in terms of true positives and true negatives but for the same reasons as explained in section **3(Discussion)**, the rates are slightly better in case of Spam versus Vs. easy-ham than in Spam versus (hard-ham + easy-ham).\n",
    "\n",
    "- Using our own list of common words to do the filtering  would be more tailor made to our case but also too specific since we compare with words that we have selected. This would take a longer time to run since we would have to loop through all emails and see if they contain a word in our list. In our method we decide how many uncommon/common words that we want to filter out and add to our list, but in sklearns algorithm you can consider all uncommon/common words that does not surpass the threshold of the parameters min_df and max_df. This results in a faster and more effective way and is also proven to work which is why we went with sklearns algorithm instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcyVfOZFU4F_"
   },
   "source": [
    "### 5. Eeking out further performance\n",
    "**a.**  Use a lemmatizer to normalize the text (for example from the `nltk` library). For one implementation look at the documentation ([here](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes)). Run your program again and answer the following questions: \n",
    "  - Why can lemmatization help?\n",
    "  -\tDoes the result improve from 3 and 4? Discuss. **1.5p** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfxBlVDVIwVu",
    "outputId": "9e5cef99-264e-49a3-ee6d-932af67bac9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Lemmatize the words in the messages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#create a lemmatizer\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bw0XCYPkOFtL",
    "outputId": "adfe56d6-8c92-444c-d6eb-ac1dbf3abef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LemmaTokenizer SPAM VS EASY-HAM\n",
      "\n",
      "Multinomial Naive Bayes Model Classifer::\n",
      "True Positive : 0.870 \n",
      "True Negative: 0.998 \n",
      "False Positive : 0.002 \n",
      "False Negative: 0.130 \n",
      "\n",
      "\n",
      "Bernoulli Naive Bayes Classifer::\n",
      "True Positive : 0.639 \n",
      "True Negative: 0.994 \n",
      "False Positive : 0.006 \n",
      "False Negative: 0.361 \n"
     ]
    }
   ],
   "source": [
    "#create a vector with the lemmatizer as the tokenizer\n",
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "\n",
    "#SPAM VS EASY-HAM\n",
    "frames_spam_easy_ham = [df1, df3] # df1 -> easy_ham and df3->spam\n",
    "all_mail_data = pd.concat(frames_spam_easy_ham)\n",
    "\n",
    "#remove words that appear are in less than 10% and more than 80% of the messages. \n",
    "email_fit = vect.fit_transform(all_mail_data['message'])    \n",
    "y = all_mail_data['message_type']\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state = 1)\n",
    "#MNB\n",
    "print(\"LemmaTokenizer SPAM VS EASY-HAM\")\n",
    "MNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "\n",
    "#BNB\n",
    "print()\n",
    "BNB_model(hamtrain, spamtrain,  hamtest, spamtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wH3SKQGdSfoG"
   },
   "outputs": [],
   "source": [
    "#create a vector with the lemmatizer as the tokenizer\n",
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "\n",
    "#SPAM VS (HARD-HAM + EASY-HAM)\n",
    "# df1 -> easy_ham and df3->spam and df2->hard_ham\n",
    "frames_spam_easy_ham_hard_ham = [df1, df2, df3]\n",
    "all_mail_data = pd.concat(frames_spam_easy_ham_hard_ham)\n",
    "#remove words that appear are in less than 10% and more than 80% of the messages. \n",
    "email_fit = vect.fit_transform(all_mail_data['message'])    \n",
    "y = all_mail_data['message_type']\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state =1)\n",
    "#MNB\n",
    "print(\"LemmaTokenizer SPAM VS (HARD-HAM + EASY-HAM)\")\n",
    "MNB_model(hamtrain, spamtrain, hamtest, spamtest)\n",
    "\n",
    "print()\n",
    "BNB_model(hamtrain, spamtrain, hamtest, spamtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMjW1mpds6PO"
   },
   "source": [
    "Lemmatization can help by recognizing different tenses of the same verb and thus, condensing the related words so we dont have much **variability** which would reduce the train vectors and affect the precision(more false positives) and recall(more true positives). This is done by reducing all the different forms of a word to one stem root. Words that are derived from each other will be mapped to the base word when having the same meaning. \n",
    "\n",
    "This is also good when you want to search for a specific word through the emails, then by condensing all the words to the steem root it would facilitate our search since we would only need to search for the stem of that word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P6hKV9x6F3K"
   },
   "source": [
    "**Discussion of results**\n",
    "\n",
    "**Spam vs easy-ham:** \n",
    "\n",
    "Using lemmatizer to normalize the text, in case of **MNB model**, we see the same true positive and true negative rates as comapred to Q3 and Q4 with an exception of true positive rates have gone down a bit by 4-5% with lemmatizer and a slight increase in False Negative. Even though this is a small difference, we don't really know the reason why MNB did somewhat poor with lemmatizer (probably how Lemmatization is optimizing the messages is making a difference).\n",
    "\n",
    "In case of **BNB model**, we see that it has performed better in terms of true positives and true negatives compared to Q3 but not as good as Q4(where we used CountVectorizer to filter out common/uncommon words). We see the similar difference in terms of false negative rates.\n",
    "\n",
    "**Spam VS (Hard-Ham + Easy-Ham):**\n",
    "\n",
    "Using lemmatizer to normalize the text, in case of **MNB model** with Spam VS (Hard-Ham + Easy-Ham), we see better performance with improved true positive and true negative rates as comapred to Q3 and Q4.\n",
    "\n",
    "In case of **BNB model** with lemmatizer, we see that it has performed better in terms of true positives and true negatives (not big difference) as compared to Q3 but not as good as Q4 (where we used CountVectorizer to filter out common/uncommon words). But we see high false negative rates as compared to Q4.\n",
    "\n",
    "\n",
    "- In conclsion based on the overall comparions in Q3, 4 and 5, we can say that  **BNB model** shows much better performance as we optimize the messages with CountVectorizer and lemmatizer (with CountVectorizer being a better option).\n",
    "On the other hand, we see almost the same performance in case of MNB model whilst doing the same optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bfd_mNtIwVu"
   },
   "source": [
    "**b) The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? **  **1p** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NODnspl-a5tY"
   },
   "source": [
    "It's because there could be an imbalance when distributing the proportions of the classes when we split the data which leads to a train and test set with different data distributions. A model that is trained on different data than the test set will lead to skewed results. \n",
    "\n",
    "The remedy to this is to use **stratification** which will lock the distributions of classes in the train and test sets. In the train_test_split there is a parameter that is called stratify where you pass the dataset with the class label. Which in our case would look like stratify=all_mail_data['message']. By adding the last parameter to our train_test_split we will now have identical distributions and our model won't need to worry about facing the problem of validation against our imbalanced test set and give us a wrong impression of the performance, which would instead happen if we would've had a training set that were mostly spam messages while our test set were mostly ham messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA2s83lJUQFW"
   },
   "source": [
    "**b) Part2 :What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages?**\n",
    "\n",
    "- if your training set were mostly spam messages while your test set were mostly ham messages, it would be probably difficult to find out weather a ham email message is ham or spam as here the used model has been only trained on features of spam emails data. We could clearly see the difference in that case if we split the test data using stratification i.e. identical distributions ( as explained in the previous section). We can check True Positives and True Negative rates for the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LMA1sAxIJU1"
   },
   "outputs": [],
   "source": [
    "frames_spam_easy_ham_hard_ham = [df1, df2, df3]\n",
    "all_mail_data = pd.concat(frames_spam_easy_ham_hard_ham)  \n",
    "email_fit = CountVectorizer().fit_transform(all_mail_data['message'])   \n",
    "y = all_mail_data['message_type']\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(email_fit,\n",
    "                                                          y,\n",
    "                                                          test_size=0.20,\n",
    "                                                          random_state =1,\n",
    "                                                          stratify=y)\n",
    "print()\n",
    "#MNB\n",
    "print(\"With Stratification::\")\n",
    "MNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "\n",
    "#BNB\n",
    "print()\n",
    "BNB_model(hamtrain, spamtrain,  hamtest, spamtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_nyGug9U4f3"
   },
   "source": [
    "**c.** Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n",
    "  - **What does this parameter mean?**\n",
    "\n",
    "  [**Answer**] Source : class sklearn.naive_bayes.MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)[source], here **fit_prior** bool, default=True\n",
    "Whether to learn class prior probabilities or not. If false, a uniform prior will be used. With **fit_prior** set to *True*, includes previously known probabilities on the dataset, which is default True and the default value is being used in this notebook so far.\n",
    "\n",
    "When set to false, it probably uses use an uniform prior with no regularization and with which we might get some inconsistent and unexpected results. We can use some other prior but not uniform ones unless we know that the bounds are representing true constraints.\n",
    "\n",
    "  - **How does this alter the predictions? Discuss why or why not.** **0.5p** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdQYfAo9IwVv"
   },
   "outputs": [],
   "source": [
    "#2 Multinomial Naive Bayes\n",
    "def MNB_model_2(hamtrain, spamtrain,  hamtest, spamtest):\n",
    "  print(\"MNB_model::\")\n",
    "  MNB = MultinomialNB(fit_prior=False)\n",
    "  MNB.fit(hamtrain, spamtrain)\n",
    "  MNB_predict = MNB.predict(hamtest) \n",
    "  tn, fp, fn, tp = confusion_matrix(spamtest, MNB_predict, normalize='true').ravel()# from matrix to array\n",
    "  matrix = classification_report(spamtest, MNB_predict) \n",
    "  return tn, fp, fn, tp\n",
    "\n",
    "tn, fp, fn, tp= MNB_model_2(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "print(\"True Positive : {:.2f} \".format(tp)) \n",
    "print(\"True Negative: {:.2f} \".format(tn))  \n",
    "#printing False rates too\n",
    "print(\"False Positive : {:.2f} \".format(fp)) \n",
    "print(\"False Negative: {:.2f} \".format(fn))\n",
    "print()\n",
    "\n",
    "\n",
    "# 2 Bernoulli Naive Bayes.\n",
    "def BNB_model_2(hamtrain, spamtrain,  hamtest, spamtest):\n",
    "  print(\"BNB_model::\")\n",
    "  BNB = BernoulliNB(fit_prior=False)\n",
    "  BNB.fit(hamtrain, spamtrain)\n",
    "  BNB_predict = BNB.predict(hamtest)\n",
    "  # true and false positive and negative\n",
    "  tn, fp, fn, tp = confusion_matrix(spamtest, BNB_predict, normalize='true').ravel()\n",
    "  matrix = classification_report(spamtest, BNB_predict)    \n",
    "  return tn, fp, fn, tp\n",
    "\n",
    "tn, fp, fn, tp = BNB_model_2(hamtrain, spamtrain,  hamtest, spamtest)\n",
    "print(\"True Positive : {:.2f} \".format(tp)) \n",
    "print(\"True Negative: {:.2f} \".format(tn))  \n",
    "#printing False rates too\n",
    "print(\"False Positive : {:.2f} \".format(fp)) \n",
    "print(\"False Negative: {:.2f} \".format(fn)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KluYkG7uZkPR"
   },
   "source": [
    "- Comparing with the results where we had the default value of *fit_prior* (True), we see that Multinomial Naive Bayes Model Classifer is performing better with improved true positive and true negative rates but for some reasons we don't see the same improvements in the Bernoulli Naive Bayes Classifer with true positives are somewhat lower than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxwp4CzNiUlL"
   },
   "source": [
    "# with default, fit_prior=True\n",
    "- MNB results with fit_prior=True\n",
    "\n",
    "True Positive : 0.897 and\n",
    "True Negative: 0.991 \n",
    "\n",
    "- BNB results with fit_prior=True\n",
    "\n",
    "True Positive : 0.320 and \n",
    "True Negative: 0.989 \n",
    "\n",
    "# with fit_prior=False\n",
    "\n",
    "- MNB results with fit_prior=False\n",
    "\n",
    "True Positive : 0.94 and \n",
    "True Negative: 0.99 \n",
    "\n",
    "- BNB results with fit_prior=False\n",
    "\n",
    "True Positive : 0.27 and\n",
    "True Negative: 0.99 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BP6Iy_RYIwVv"
   },
   "source": [
    "**d.** **The python model includes smoothing (`alpha` parameter ), explain why this can be important.** \n",
    "  - **What would happen if in the training data set the word 'money' only appears in spam examples? What would the model predict about a message containing the word 'money'? Does the prediction depend on the rest of the message and is that reasonable? Explain your reasoning**  **1p** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUlvQDzBtlx3"
   },
   "source": [
    "-  alphafloat, default=1.0 :: Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "- It represents the additive smoothing parameter. If you choose 0 as its value, then there will be no smoothing. If we choose a value of alpha!=0 (not equal to 0), the probability will no longer be zero even if a word is not present in the training dataset.\n",
    "\n",
    "As alpha increases, the likelihood probability moves towards uniform distribution. Most of the time, alpha = 1 is being used to remove the problem of zero probability. The laplace smoothing is a smoothing technique that helps to tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. It preferred to use alpha=1.\n",
    "\n",
    "\n",
    "if in the training data set the word '**money**',  and since it only appears in spam, true negative rate would increase. The predications can vary based on the training data set proportion. If we do it using stratication with equal proportion then it would matter that much. The prediction would not depend on on the rest of the message as the model is trained to differentiate based on the word \"Money\" here ( not anything else) and this is not reasonable since we need to take into account the rest of the words in the message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND6FKoexVAhW"
   },
   "source": [
    "### What to report and how to hand in.\n",
    "\n",
    "- You will need to clearly report all results in the notebook in a clear and appropriate way, either using plots or code output (f.x. \"print statements\"). \n",
    "- The notebook must be reproducible, that means, we must be able to use the `Run all` function from the `Runtime` menu and reproduce all your results. **Please check this before handing in.** \n",
    "- Save the notebook and share a link to the notebook (Press share in upper left corner, and use `Get link` option. **Please make sure to allow all with the link to open and edit.**\n",
    "- Edits made after submission deadline will be ignored, graders will recover the last saved version before deadline from the revisions history.\n",
    "- **Please make sure all cells are executed and all the output is clearly readable/visible to anybody opening the notebook.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
